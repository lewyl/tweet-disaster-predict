{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4788d0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186c272",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa7a8932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path = \"./train.csv\"\n",
    "df_train = pd.read_csv(train_data_path)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd52b57",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "### Examine Target Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab0fdb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.value_counts(df_train['target']).plot.bar()\n",
    "plt.title('Distribution')\n",
    "plt.xlabel('label')\n",
    "plt.ylabel('counts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b1a520",
   "metadata": {},
   "source": [
    "The dataset is slightly imbalanced, with more tweets not about real disaster. Hence, data augmentation is performed to generate tweets with label 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbca7d7",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a692e1b5",
   "metadata": {},
   "source": [
    "Before data augmentation, data is split into train and test set with ratio 80:20. This is because validation set should not contain augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69d0f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val = train_test_split(df_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfe5e88",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "Data augmentation was done by **synonym replacement**. \n",
    "This is done by randomly sample words that are not stop words and replace them with their corresponding synonyms. \n",
    "For example, “This exam is pretty hard.” is augmented to “This test is pretty difficult.”\n",
    "\n",
    "### Word embeddings\n",
    "To know whether a pair of words are synonyms, GloVe word embeddings are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "696c42a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load glove vectors into dictionary\n",
    "emb_path = \"./glove.6B.100d.txt\"\n",
    "\n",
    "glove_model = {}\n",
    "with open(emb_path,'r') as f:\n",
    "    for line in f:\n",
    "        items = line.split()\n",
    "        word = items[0]\n",
    "        emb = np.array(items[1:], dtype=np.float64)\n",
    "        glove_model[word] = emb.astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9882c54e",
   "metadata": {},
   "source": [
    "### Data augmentation on class 1\n",
    "The synonym replacement was coded manually without packages. From the training data, 300 tweets were sampled to generate augmented data. Each tweet has two words replaced using their synonyms.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "206175c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3237</th>\n",
       "      <td>4649</td>\n",
       "      <td>engulfed</td>\n",
       "      <td>Fredonia,NY</td>\n",
       "      <td>Just saw a car on the I-77 Fully engulfed in f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>2047</td>\n",
       "      <td>casualties</td>\n",
       "      <td>everywhere</td>\n",
       "      <td>Revise the Death to America scenario? \\n\\nWhil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>197</td>\n",
       "      <td>airplane%20accident</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>Strict liability in the context of an airplane...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4272</th>\n",
       "      <td>6070</td>\n",
       "      <td>heat%20wave</td>\n",
       "      <td>International Action</td>\n",
       "      <td>Heat wave adding to the misery of internally-d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7331</th>\n",
       "      <td>10492</td>\n",
       "      <td>wildfire</td>\n",
       "      <td>Amsterdam | San Francisco</td>\n",
       "      <td>'Some hearths they burn like a wildfire' https...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id              keyword                   location  \\\n",
       "3237   4649             engulfed                Fredonia,NY   \n",
       "1419   2047           casualties                everywhere    \n",
       "137     197  airplane%20accident               Pennsylvania   \n",
       "4272   6070          heat%20wave       International Action   \n",
       "7331  10492             wildfire  Amsterdam | San Francisco   \n",
       "\n",
       "                                                   text  target  \n",
       "3237  Just saw a car on the I-77 Fully engulfed in f...       1  \n",
       "1419  Revise the Death to America scenario? \\n\\nWhil...       1  \n",
       "137   Strict liability in the context of an airplane...       1  \n",
       "4272  Heat wave adding to the misery of internally-d...       1  \n",
       "7331  'Some hearths they burn like a wildfire' https...       1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "# returns top 5 closest embeddings excluding self\n",
    "def find_closest_emb(emb):\n",
    "    return sorted(glove_model.keys(), key=lambda word: spatial.distance.euclidean(glove_model[word], emb))[1:6]\n",
    "\n",
    "# source dataframe to generate augmentations\n",
    "df_aug_source = X_train[X_train['target']==1]\n",
    "df_aug_source.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9daea9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BigRigRadio Live Accident Awareness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Policyholders object help Clico rescue plan ht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@SoonerMagic_ I means I'm a fan but I don't ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>THE GLOBAL ECONOMIC MELTDOWN is out! http://t....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NWS says thunderstorms with deadly lightning w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                BigRigRadio Live Accident Awareness\n",
       "1  Policyholders object help Clico rescue plan ht...\n",
       "2  @SoonerMagic_ I means I'm a fan but I don't ne...\n",
       "3  THE GLOBAL ECONOMIC MELTDOWN is out! http://t....\n",
       "4  NWS says thunderstorms with deadly lightning w..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_aug(text):\n",
    "    words = list(text.split())\n",
    "    index = list(np.random.randint(0, high=len(words), size=5)) # random index to swap with synonym\n",
    "    swap = 0\n",
    "    for i in index:\n",
    "        if words[i] in glove_model.keys():\n",
    "            top_5 = find_closest_emb(glove_model[words[i]])\n",
    "            j = np.random.randint(0, high=5) # randomly choose a synonym from top 5 closest embeddings\n",
    "            words[i] = top_5[j]\n",
    "            swap += 1\n",
    "            if swap == 2:\n",
    "                break\n",
    "    text_augment = ' '.join(words)\n",
    "    return text_augment\n",
    "\n",
    "def generate_aug_samples(data, num_samples = 300):\n",
    "    aug_data = []\n",
    "    index = np.random.randint(0, high=len(data), size=num_samples) # random indexes to generate augmented data from\n",
    "    for i in index:\n",
    "        text = data.iloc[i]\n",
    "        text_augment = generate_aug(text)\n",
    "        aug_data.append(text_augment)\n",
    "    return pd.DataFrame(aug_data, columns=['text'])\n",
    "    \n",
    "# generate augmented data\n",
    "df_aug = generate_aug_samples(df_aug_source['text']) \n",
    "df_aug.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43c3df78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aug['target'] = 1 # add label to augmented data\n",
    "df_aug.to_csv('Q6_generated.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9675970b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5237</th>\n",
       "      <td>For maximum damage! Activate [BIG BAND MODE] f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>HIS MAJESTY EMPEROR SALMAN KHAN'S UNSTOPPABLE ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3237</th>\n",
       "      <td>Just saw a car on the I-77 Fully engulfed in f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2712</th>\n",
       "      <td>Ignition Knock (Detonation) Sensor-Senso Stand...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3904</th>\n",
       "      <td>Imagine getting flattened by Kurt Zouma</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target\n",
       "5237  For maximum damage! Activate [BIG BAND MODE] f...       0\n",
       "1973  HIS MAJESTY EMPEROR SALMAN KHAN'S UNSTOPPABLE ...       0\n",
       "3237  Just saw a car on the I-77 Fully engulfed in f...       1\n",
       "2712  Ignition Knock (Detonation) Sensor-Senso Stand...       0\n",
       "3904            Imagine getting flattened by Kurt Zouma       0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df_train = pd.concat([X_train[['text','target']], df_aug])\n",
    "new_df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0543ab1b",
   "metadata": {},
   "source": [
    "## Tweet Prediction\n",
    "Train a machine learning model to predict whether a Tweet is about real disasters or not. Two models with the same architecture and hyperparameters are trained on plain data and augmented data respectively.\n",
    "\n",
    "### Build text embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a99a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Dropout, Input, SpatialDropout1D, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46458e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19384"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize train data\n",
    "tokenizer = Tokenizer(num_words=2000, split=' ')\n",
    "tokenizer.fit_on_texts(X_train['text'])\n",
    "\n",
    "train_tweets = tokenizer.texts_to_sequences(X_train['text'].astype(str).values)\n",
    "max_len = max([len(i) for i in train_tweets])\n",
    "train_tweets = pad_sequences(train_tweets, maxlen = max_len)\n",
    "val_tweets = tokenizer.texts_to_sequences(X_val['text'].astype(str).values)\n",
    "val_tweets = pad_sequences(val_tweets, maxlen = max_len)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941be888",
   "metadata": {},
   "source": [
    "### Baseline LSTM model (without data augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "295f4765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 29, 256)           512000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_4 (Spatial (None, 29, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 1,037,826\n",
      "Trainable params: 1,037,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Build model\n",
    "# model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(2000, 256, input_length = train_tweets.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(256, dropout = 0.2))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['AUC'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d29a8f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:4211: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n",
      "/anaconda/envs/azureml_py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:4211: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.6540 - auc: 0.6634\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.5609 - auc: 0.7866\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.4322 - auc: 0.8819\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.3788 - auc: 0.9108\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.3436 - auc: 0.9272\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 1s 63ms/step - loss: 0.3123 - auc: 0.9395\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.2957 - auc: 0.9459\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.2790 - auc: 0.9519\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.2616 - auc: 0.9579\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.2475 - auc: 0.9625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f59dc040550>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# training on plain data\n",
    "model.fit(train_tweets,\n",
    "          pd.get_dummies(X_train['target'].values),\n",
    "          batch_size=512,\n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6224b5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 27ms/step - loss: 0.5977 - auc: 0.8579\n",
      "Validation auc: 0.8578559756278992\n"
     ]
    }
   ],
   "source": [
    "# validation\n",
    "score, auc = model.evaluate(val_tweets, pd.get_dummies(X_val['target']).values, batch_size = 512)\n",
    "print(\"Validation auc: {}\".format(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d14f72",
   "metadata": {},
   "source": [
    "### LSTM model with augmented data\n",
    "The same architecture is used for both plain and augmented data. The models were trained with batch size 512 and 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f8fc7e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19451"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize train data\n",
    "tokenizer = Tokenizer(num_words=2000, split=' ')\n",
    "tokenizer.fit_on_texts(new_df_train['text'])\n",
    "\n",
    "train_tweets = tokenizer.texts_to_sequences(X_train['text'].astype(str).values)\n",
    "max_len = max([len(i) for i in train_tweets])\n",
    "train_tweets = pad_sequences(train_tweets, maxlen = max_len)\n",
    "val_tweets = tokenizer.texts_to_sequences(X_val['text'].astype(str).values)\n",
    "val_tweets = pad_sequences(val_tweets, maxlen = max_len)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b0e12e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 29, 256)           512000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_5 (Spatial (None, 29, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 1,037,826\n",
      "Trainable params: 1,037,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Build model\n",
    "\n",
    "# model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(2000, 256, input_length = train_tweets.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(256, dropout = 0.2))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['AUC'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "346648a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 2/12 [====>.........................] - ETA: 0s - loss: 0.6878 - auc_1: 0.5738"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38_tensorflow/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:4211: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 66ms/step - loss: 0.6530 - auc_1: 0.6619\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 1s 59ms/step - loss: 0.5563 - auc_1: 0.7916\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.4313 - auc_1: 0.8819\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.3695 - auc_1: 0.9153\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 1s 60ms/step - loss: 0.3336 - auc_1: 0.9310\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.3072 - auc_1: 0.9415\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 1s 65ms/step - loss: 0.2916 - auc_1: 0.9474\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 1s 62ms/step - loss: 0.2772 - auc_1: 0.9529\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 1s 66ms/step - loss: 0.2641 - auc_1: 0.9572\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 1s 61ms/step - loss: 0.2571 - auc_1: 0.9597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f59f66219a0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training on augmented data\n",
    "model.fit(train_tweets,\n",
    "          pd.get_dummies(X_train['target'].values),\n",
    "          batch_size=512,\n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7bb7d1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 27ms/step - loss: 0.5657 - auc_1: 0.8604\n",
      "Validation auc: 0.8604108691215515\n"
     ]
    }
   ],
   "source": [
    "# validation\n",
    "score, auc = model.evaluate(val_tweets, pd.get_dummies(X_val['target']).values, batch_size = 512)\n",
    "print(\"Validation auc: {}\".format(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ac4f3e",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Data augmentation has improved the performance of the tweet prediction model. With data augmentation, class imbalanced problem can be tackled to improve prediction model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c22553b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "azureml_py38_tensorflow"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
